# Design space from BERT-tiny to BERT-small
# Nine pre-trained models available at googl-research/bert

datasets:
  - CoLA
  - SST-2
  - MRPC
  - STS-B
  - QQP
  - MNLI-mm
  - QNLI
  - RTE
  - WNLI

architecture:
  hidden_size:
    - 128
    - 256
    - 512
  num_heads:
    - 2
    - 4
    - 8
  encoder_layers:
    - 2
    - 4
    - 6
  operation_types:
    - sa
    - l
    - c
  number_of_feed-forward_stacks:
    - 1
    - 2 # An extra
    - 3 # An extra
  feed-forward_hidden:
    - 512
    - 1024
    - 2048
    - 4096 # An extra
  operation_parameters:
    sa:
      - sdp
      - wma # An extra
    l:
      - dft # An extra
      - dct # An extra
    c:
      - 5 # An extra
      - 9 # An extra
      - 13 # An extra
